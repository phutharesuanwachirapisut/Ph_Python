{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx8Q19d7BYyx"
      },
      "source": [
        "## ตัวอย่าง DataPreprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQWGlVzxQ00d"
      },
      "source": [
        "ติดตั้ง Library พื้นฐาน Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "K91X0m2uCilI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download('all')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtHWKbe9Cd20"
      },
      "source": [
        "ตัวอย่างการทำ sentence และ word tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "bNGdSbBPRi9p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: This is a cat. It’s so cute! \n",
            "['This is a cat.', 'It’s so cute!']\n",
            "['This', 'is', 'a', 'cat', '.', 'It', '’', 's', 'so', 'cute', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk import  word_tokenize, sent_tokenize\n",
        "\n",
        "\n",
        "data = \"This is a cat. It’s so cute! \"\n",
        "print(f\"Data: {data}\")\n",
        "result_sent_tokenized =   sent_tokenize(data)# sent_tokenize ตัดข้อความ เป็นประโยค\n",
        "print(result_sent_tokenized)\n",
        "\n",
        "result_word_tokenized =   word_tokenize(data) # word_tokenize ตัดข้อความ เป็นคำ\n",
        "print(result_word_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnWik8b7Sz0G"
      },
      "source": [
        "สร้างตัวอย่างข้อมูลเบื้องต้นและจัดเก็บใน Data Frame ด้วย Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "iBdEOrBLStx3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          Content\n",
            "0               An introduction to text analysis.\n",
            "1     These're the best books for text analysis!!\n",
            "2                            This book costs 40$.\n",
            "3                         I am interested in NLP.\n",
            "4           Machine learning is very interesting.\n",
            "5     I'd like to introduce Mark, Analyst expert!\n",
            "6         Link : https://www.w3schools.com/python\n",
            "7  I've started to learn Python when I was young.\n",
            "8              Several books are written by John.\n",
            "9                              They're scientist.\n"
          ]
        }
      ],
      "source": [
        "text=[\"An introduction to text analysis.\",\n",
        "      \"These're the best books for text analysis!!\",\n",
        "      \"This book costs 40$.\",\n",
        "      \"I am interested in NLP.\",\n",
        "      \"Machine learning is very interesting.\",\n",
        "      \"I'd like to introduce Mark, Analyst expert!\",\n",
        "      \"Link : https://www.w3schools.com/python\",\n",
        "      \"I've started to learn Python when I was young.\",\n",
        "      \"Several books are written by John.\",\n",
        "      \"They're scientist.\"]\n",
        "\n",
        "#convert list to data frame\n",
        "import pandas as pd\n",
        "data = pd.DataFrame({'Content':text})\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h5Vz-bEVppo"
      },
      "source": [
        "กำจัด URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lgDaaL0nVvC9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          Content\n",
            "0               An introduction to text analysis.\n",
            "1     These're the best books for text analysis!!\n",
            "2                            This book costs 40$.\n",
            "3                         I am interested in NLP.\n",
            "4           Machine learning is very interesting.\n",
            "5     I'd like to introduce Mark, Analyst expert!\n",
            "6                                         Link : \n",
            "7  I've started to learn Python when I was young.\n",
            "8              Several books are written by John.\n",
            "9                              They're scientist.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "data['Content']  = data['Content'].apply(lambda x:\" \".join([re.sub(r\"https?://[A-Za-z0-9./]+\", '', w ) for w in x.split()])) \n",
        "\t# แต่ละบรรทัด split ตามช่องว่างแล้วนำมาสร้างเป็น list\n",
        "    # แล้วเอาแต่ละสมาชิกใน list มาพิจารณาว่าส่วนตรงไหนที่มี pattern แบบนี้ https?://[A-Za-z0-9./]+ ให้แทนที่ด้วย '' (ช่องว่าง)\n",
        "    # ขั้นตอนสุดท้ายเอาสมาชิก list ทั้งหมดที่ถูก split นำมาต่อกัน\n",
        "print(data)\n",
        "\n",
        "# สังเกต index = 6 จะเหลือแค่ Link :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9hYGRuwX9qZ"
      },
      "source": [
        "เตรียมข้อมูลสำหรับทดลอง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2YNTSRxvX996"
      },
      "outputs": [],
      "source": [
        "data_Stemming_PorterStemmer = data.copy()\n",
        "data_Stemming_SnowballStemmer = data.copy()\n",
        "\n",
        "data_Lemma_textblob = data.copy()\n",
        "data_Lemma_spacy = data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2t6YPNzYVYG"
      },
      "source": [
        "# Stemming (การตัดส่วนขยาย)\n",
        "เช่น ตัด s, es, ing หรือ ed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQMTw9EAYorV"
      },
      "source": [
        "*(1) Stemming ด้วย PorterStemmer* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CB0NLKsiYYui"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      Content\n",
            "0              an introduct to text analysis.\n",
            "1   these'r the best book for text analysis!!\n",
            "2                          thi book cost 40$.\n",
            "3                       i am interest in nlp.\n",
            "4           machin learn is veri interesting.\n",
            "5  i'd like to introduc mark, analyst expert!\n",
            "6                                      link :\n",
            "7  i'v start to learn python when i wa young.\n",
            "8             sever book are written by john.\n",
            "9                           they'r scientist.\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "data_Stemming_PorterStemmer['Content'] = data_Stemming_PorterStemmer['Content'].apply(lambda x: \" \".join([porter.stem(w) for w in x.split()]))\n",
        "print(data_Stemming_PorterStemmer)\n",
        "\n",
        "# ตัดแบบห้วน ๆ \n",
        "\t# machine -> machin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dhhmDzsY2tt"
      },
      "source": [
        "*(2) Stemming ด้วย SnowballStemmer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "s0rI8OgCY3Mf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         Content\n",
            "0                 an introduct to text analysi .\n",
            "1    these re the best book for text analysi ! !\n",
            "2                          this book cost 40 $ .\n",
            "3                         i am interest in nlp .\n",
            "4                machin learn is veri interest .\n",
            "5  i 'd like to introduc mark , analyst expert !\n",
            "6                                         link :\n",
            "7  i ve start to learn python when i was young .\n",
            "8               sever book are written by john .\n",
            "9                            they re scientist .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "data_Stemming_SnowballStemmer['Content'] = data_Stemming_SnowballStemmer['Content'].apply(lambda x: \" \".join([snowball.stem(w) for w in  word_tokenize(x)]))\n",
        "print(data_Stemming_SnowballStemmer)\n",
        "\n",
        "# they'r -> they re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd7ugOYqMlW7"
      },
      "source": [
        "# Lemmatization (การเปลี่ยนรูปคำให้อยู่ในรูปแบบดั้งเดิม)\n",
        "*   am, are, is\n",
        "*   was ‐> be\n",
        "*   saw, seen ‐> see\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onqt2PH5agSf"
      },
      "source": [
        "*(1) Lemmatization ด้วย Textblob*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-CxJomeFaqj7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          Content\n",
            "0               An introduction to text analysis.\n",
            "1    These're the best books for text analysis! !\n",
            "2                            This book costs 40$.\n",
            "3                         I am interested in NLP.\n",
            "4           Machine learning is very interesting.\n",
            "5     I'd like to introduce Mark, Analyst expert!\n",
            "6                                          Link :\n",
            "7  I've started to learn Python when I was young.\n",
            "8              Several books are written by John.\n",
            "9                              They're scientist.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/phutharesuanwachirapisut/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/phutharesuanwachirapisut/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from textblob import Word\n",
        "\n",
        "data_Lemma_textblob['Content'] = data_Lemma_textblob['Content'].apply(lambda x: \" \".join([Word(w).lemmatize() for w in  sent_tokenize(x)]))\n",
        "print(data_Lemma_textblob)\n",
        "# ตัดทีละคำ แล้วเอาไปทำ lemmatize "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYXmHpbJazVb"
      },
      "source": [
        "*(2) Lemmatization ด้วย Spacy*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ejfXpZtHa6M9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Content\n",
            "0                 an introduction to text analysis .\n",
            "1       these be the good book for text analysis ! !\n",
            "2                                this book cost 40$.\n",
            "3                           I be interested in NLP .\n",
            "4             machine learning be very interesting .\n",
            "5  I would like to introduce Mark , Analyst expert !\n",
            "6                                             link :\n",
            "7     I have start to learn Python when I be young .\n",
            "8                    several book be write by John .\n",
            "9                                they be scientist .\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "data_Lemma_spacy['Content'] = data_Lemma_spacy['Content'].apply(lambda x: \" \".join([w.lemma_ for w in nlp(x) ]))\n",
        "print(data_Lemma_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lemmatization ด้วย Spacy ดีสุด จึงเอามาใช้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Rc7uq9qlunkq"
      },
      "outputs": [],
      "source": [
        "#data_Lemma_spacy['Content'] = data_Lemma_spacy['Content'].apply(lambda x: \" \".join([w.lemma_ for w in nlp(x) if w.lemma_ != \"-PRON-\"]))\n",
        "#print(data_Lemma_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Mms5loGDSuYW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Content\n",
            "0                 an introduction to text analysis .\n",
            "1       these be the good book for text analysis ! !\n",
            "2                                this book cost 40$.\n",
            "3                           I be interested in NLP .\n",
            "4             machine learning be very interesting .\n",
            "5  I would like to introduce Mark , Analyst expert !\n",
            "6                                             link :\n",
            "7     I have start to learn Python when I be young .\n",
            "8                    several book be write by John .\n",
            "9                                they be scientist .\n"
          ]
        }
      ],
      "source": [
        "data = data_Lemma_spacy.copy() \n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4ELhtUsSdMK"
      },
      "source": [
        "แปลงข้อมูลเป็นตัวพิมพ์เล็ก ... lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "bQME4WquSQwf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Content\n",
            "0                 an introduction to text analysis .\n",
            "1       these be the good book for text analysis ! !\n",
            "2                                this book cost 40$.\n",
            "3                           i be interested in nlp .\n",
            "4             machine learning be very interesting .\n",
            "5  i would like to introduce mark , analyst expert !\n",
            "6                                             link :\n",
            "7     i have start to learn python when i be young .\n",
            "8                    several book be write by john .\n",
            "9                                they be scientist .\n"
          ]
        }
      ],
      "source": [
        "data['Content'] = data['Content'].apply(lambda x:\" \".join([w.lower() for w in x.split()]))\n",
        "print(data)\n",
        "\n",
        "# แปลงเป็นตัวพิมพ์เล็ก"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBziiksEFCf"
      },
      "source": [
        "กำจัด stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "olLwkCfISJtL"
      },
      "outputs": [],
      "source": [
        "# nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "R7zNU626EJZn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    Content\n",
            "0                introduction text analysis\n",
            "1                   good book text analysis\n",
            "2                                 book cost\n",
            "3                            interested nlp\n",
            "4              machine learning interesting\n",
            "5  would like introduce mark analyst expert\n",
            "6                                      link\n",
            "7                  start learn python young\n",
            "8                   several book write john\n",
            "9                                 scientist\n"
          ]
        }
      ],
      "source": [
        "stop = stopwords.words('english')\n",
        "data['Content'] = data['Content'].apply(lambda x: \" \".join(w for w in word_tokenize(x) if w not in stop))\n",
        "print(data)\n",
        "# เอาแต่ละไปดูว่าอยู่ ใน stop word ไหม ถ้าไม่อยู่ ให้เอามา join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_PvLFHDwHT"
      },
      "source": [
        "ตัดเครื่องหมายวรรคตอน ... remove punctuation (Special Charactor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zcByTZJ5Dx4n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    Content\n",
            "0                introduction text analysis\n",
            "1                   good book text analysis\n",
            "2                              book cost 40\n",
            "3                            interested nlp\n",
            "4              machine learning interesting\n",
            "5  would like introduce mark analyst expert\n",
            "6                                      link\n",
            "7                  start learn python young\n",
            "8                   several book write john\n",
            "9                                 scientist\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "data['Content'] = data['Content'].apply(lambda x: \" \".join([w for w in x.split() if w not in list(string.punctuation)]))\n",
        "print(data)\n",
        "# เอาแต่ละไปดูว่าอยู่ ใน list special char. ไหม ถ้าไม่อยู่ ให้เอามา join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_1GaGcQCRHQ"
      },
      "source": [
        "กำจัดตัวเลข"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "R3RBReoACLMj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    Content\n",
            "0                introduction text analysis\n",
            "1                   good book text analysis\n",
            "2                                 book cost\n",
            "3                            interested nlp\n",
            "4              machine learning interesting\n",
            "5  would like introduce mark analyst expert\n",
            "6                                      link\n",
            "7                  start learn python young\n",
            "8                   several book write john\n",
            "9                                 scientist\n"
          ]
        }
      ],
      "source": [
        "data['Content']  = data['Content'].apply(lambda x:\" \".join(re.sub(r\"[0-9.]+\", '', w ) for w in x.split())) \n",
        "print(data)\n",
        "# เอาแต่ละไปดูว่าอยู่ ใน pattern (0 ถึง 9) ที่ปรากฎไหม ถ้าอยู่ ให้แทนที่ด้วย '' (ใช้ function sub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkId0Ih5CVIN"
      },
      "source": [
        "กำจัดข้อความหลัง ' เครื่องหมาย apostrophe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "kcAmtb_yCYDG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    Content\n",
            "0                introduction text analysis\n",
            "1                   good book text analysis\n",
            "2                                 book cost\n",
            "3                            interested nlp\n",
            "4              machine learning interesting\n",
            "5  would like introduce mark analyst expert\n",
            "6                                      link\n",
            "7                  start learn python young\n",
            "8                   several book write john\n",
            "9                                 scientist\n"
          ]
        }
      ],
      "source": [
        "data['Content']  = data['Content'].apply(lambda x:\" \".join(re.sub(r\"(')[A-Za-z0-9.]+\", '', w ) for w in x.split())) \n",
        "print(data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
