{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx8Q19d7BYyx"
   },
   "source": [
    "## ตัวอย่างการทำ Text classification\n",
    "เพื่อจำแนกเอกสารประเภท Spam Mail\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQWGlVzxQ00d"
   },
   "source": [
    "ติดตั้ง Libraries สำหรับจัดการข้อมูลประเภทข้อความภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K91X0m2uCilI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/phutharesuanwachirapisut/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/phutharesuanwachirapisut/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import  word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnWik8b7Sz0G"
   },
   "source": [
    "ทำการเชื่อม Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBdEOrBLStx3",
    "outputId": "dd83325f-eff9-4c0c-cde4-f1ed662bd7f4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSfUW4UlEMn4"
   },
   "source": [
    "ระบุ Folder ใน Drive ที่ต้องการใช้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v934qbfOEOHp"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"/content/gdrive/My Drive/Colab Notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzBiBg8VEP5h"
   },
   "source": [
    "อ่านข้อมูลจากไฟล์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U0_QiYTkEUCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Class                                            Content\n",
      "0     ham     Not really dude, have no friends i'm afraid :(\n",
      "1     ham                            Coffee cake, i guess...\n",
      "2     ham  Merry Christmas to you too babe, i love ya *ki...\n",
      "3     ham  Hey... Why dont we just go watch x men and hav...\n",
      "4     ham  cud u tell ppl im gona b a bit l8 cos 2 buses ...\n",
      "..    ...                                                ...\n",
      "456  spam  This msg is for your mobile content order It h...\n",
      "457  spam    You have 1 new message. Please call 08715205273\n",
      "458  spam  December only! Had your mobile 11mths+? You ar...\n",
      "459  spam  Get 3 Lions England tone, reply lionm 4 mono o...\n",
      "460  spam       PRIVATE! Your 2003 Account Statement for 078\n",
      "\n",
      "[461 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"spam.csv\",encoding='latin-1')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h5Vz-bEVppo"
   },
   "source": [
    "# Data Preprocessing  สร้างเป็นฟังก์ชันเพื่อเรียกใช้ซ้ำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Aw256l6Gv95T"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(cleaned_data):\n",
    "    # กำจัด URL\n",
    "    cleaned_data = \" \".join(re.sub(r\"https?://[A-Za-z0-9./]+\", '', w ) for w in cleaned_data.split()) \n",
    "\n",
    "    # แปลงข้อมูลเป็นตัวพิมพ์เล็ก ... lower case\n",
    "    cleaned_data = \" \".join(w.lower() for w in cleaned_data.split())\n",
    "\n",
    "    # Lemmatization ด้วย Spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    cleaned_data =  \" \".join([w.lemma_ for w in nlp(cleaned_data) if w.lemma_ != \"-PRON-\"])\n",
    "\n",
    "    # กำจัด stop words\n",
    "    stop = stopwords.words('english')\n",
    "    cleaned_data = \" \".join(w for w in word_tokenize(cleaned_data) if w not in stop)\n",
    "\n",
    "    # ตัดเครื่องหมายวรรคตอน ... remove punctuation\n",
    "    cleaned_data =  \" \".join([w for w in cleaned_data.split() if w not in list(string.punctuation)])\n",
    "\n",
    "    # กำจัดตัวเลข\n",
    "    cleaned_data = \" \".join(re.sub(r\"[0-9.]+%\", '', w ) for w in cleaned_data.split()) \n",
    "\n",
    "    # กำจัดข้อความหลัง ' เครื่องหมาย aposophi\n",
    "    cleaned_data = \" \".join(re.sub(r\"(')[A-Za-z]+\", '', w ) for w in cleaned_data.split())\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOx6ShWZwXDL"
   },
   "source": [
    "เรียกใช้งานฟังก์ชัน data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1CYS3c4wZB7"
   },
   "outputs": [],
   "source": [
    "data['Content'] = data['Content'].apply(data_preprocessing)\n",
    "# แสดงผลข้อมูลหลังจาก Data Preprocessing\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3FIFZ9YCsMY"
   },
   "source": [
    "# สร้าง TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH_bLKmgCsYR"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y = encoder.fit_transform(data['Class'])\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "tfidf_vect.fit(data['Content'])\n",
    "x_tfidf = tfidf_vect.transform(data['Content'])\n",
    "print(x_tfidf[0:5] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OlmBpeHIcZH"
   },
   "source": [
    "สร้าง Training และ Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydPsWmCsHcej"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and validation\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(x_tfidf, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmlqGEKvIBdK"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "# Model building and evaluation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models = []\n",
    "models.append(('NB', MultinomialNB()))\n",
    "models.append(('Log-reg', LogisticRegression()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('RandomForest', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, clf in models:\n",
    "  model = clf\n",
    "  model.fit(X_train,y_train)\n",
    "  Predited_class = model.predict(X_test)\n",
    "  accuracy=accuracy_score(y_test, Predited_class)\n",
    "  results.append(accuracy)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f \" % (name, accuracy)\n",
    "  print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.bar(names,results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EswV0TgGeZ9C"
   },
   "source": [
    "# Apply the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mrt50jZbSB8"
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxOLZu8CdUHZ"
   },
   "source": [
    "Appication form for data classifcaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yt4381h3bbla"
   },
   "outputs": [],
   "source": [
    "#@title Enter Content\n",
    "Content = \"I  do not like it\" #@param {type:\"string\"}\n",
    "# use the classifier to predict the sentiment of given text\n",
    "Content = data_preprocessing(Content)\n",
    "xtest_tfidf = tfidf_vect.transform([Content])\n",
    "result = model.predict(xtest_tfidf)\n",
    "\n",
    "result = encoder.inverse_transform(result)\n",
    "if result[0] == 'spam':\n",
    "      print(\"This is a spam message.\")\n",
    "elif result[0] == 'ham':\n",
    "      print(\"This is not a spam message.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
