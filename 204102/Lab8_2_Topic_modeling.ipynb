{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx8Q19d7BYyx"
      },
      "source": [
        "## ตัวอย่างการทำ Topic Modeling\n",
        "เพื่อทำสรุปหัวข้อเอกสาร\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ติดตั้ง Libraries สำหรับจัดการข้อมูลประเภทข้อความภาษาอังกฤษ"
      ],
      "metadata": {
        "id": "GQWGlVzxQ00d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install --upgrade pyLDAvis"
      ],
      "metadata": {
        "id": "pdSejpst_JKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import  word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import  LatentDirichletAllocation\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n"
      ],
      "metadata": {
        "id": "K91X0m2uCilI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "MY7bncQNDTOC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ทำการเชื่อม Google Drive"
      ],
      "metadata": {
        "id": "EnWik8b7Sz0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "iBdEOrBLStx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e72ba3c-03e0-4fb4-eb34-a1a7c750da85"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ระบุ Folder ใน Drive ที่ต้องการใช้"
      ],
      "metadata": {
        "id": "YSfUW4UlEMn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "metadata": {
        "id": "v934qbfOEOHp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "อ่านข้อมูลจากไฟล์"
      ],
      "metadata": {
        "id": "DzBiBg8VEP5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"SampleTopics.csv\",encoding='latin-1')\n",
        "print(data )"
      ],
      "metadata": {
        "id": "U0_QiYTkEUCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing  สร้างเป็นฟังก์ชันเพื่อเรียกใช้ซ้ำ"
      ],
      "metadata": {
        "id": "3h5Vz-bEVppo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(cleaned_data):\n",
        "    # กำจัด URL\n",
        "    cleaned_data = \" \".join(re.sub(r\"https?://[A-Za-z0-9./]+\", '', w ) for w in cleaned_data.split())\n",
        "\n",
        "    # แปลงข้อมูลเป็นตัวพิมพ์เล็ก ... lower case\n",
        "    cleaned_data = \" \".join(w.lower() for w in cleaned_data.split())\n",
        "\n",
        "    # Lemmatization ด้วย Spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    cleaned_data =  \" \".join([w.lemma_ for w in nlp(cleaned_data) if w.lemma_ != \"-PRON-\"])\n",
        "\n",
        "    # กำจัด stop words\n",
        "    stop = stopwords.words('english')\n",
        "    cleaned_data = \" \".join(w for w in word_tokenize(cleaned_data) if w not in stop)\n",
        "\n",
        "     # กำจัด stop words\n",
        "    stop = stopwords.words('english')\n",
        "    cleaned_data = \" \".join(w for w in word_tokenize(cleaned_data) if w not in ('study'))\n",
        "\n",
        "    # ตัดเครื่องหมายวรรคตอน ... remove punctuation\n",
        "    cleaned_data =  \" \".join([w for w in cleaned_data.split() if w not in list(string.punctuation)])\n",
        "\n",
        "    # กำจัดตัวเลข\n",
        "    cleaned_data = \" \".join(re.sub(r\"[0-9]+\", '', w ) for w in cleaned_data.split())\n",
        "\n",
        "    # กำจัดข้อความหลัง ' เครื่องหมาย aposophi\n",
        "    cleaned_data = \" \".join(re.sub(r\"(')[A-Za-z]+\", '', w ) for w in cleaned_data.split())\n",
        "\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "Aw256l6Gv95T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เรียกใช้งานฟังก์ชัน data_preprocessing"
      ],
      "metadata": {
        "id": "HOx6ShWZwXDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Content'] = data['Content'].apply(data_preprocessing)\n",
        "\n",
        "# แสดงผลข้อมูลหลังจาก Data Preprocessing\n",
        "print(data)"
      ],
      "metadata": {
        "id": "y1CYS3c4wZB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# สร้าง document term matrix"
      ],
      "metadata": {
        "id": "l3FIFZ9YCsMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(data['Content'])\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "print(tf_feature_names)"
      ],
      "metadata": {
        "id": "ji8wfk73BuaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e84f16-10c9-4fb9-c969-cdd27f2b8361"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adaptation' 'algorithm' 'ancient' 'animal' 'argue' 'aristotle' 'atom'\n",
            " 'behavior' 'biology' 'bond' 'chemical' 'chemistry' 'compound' 'computer'\n",
            " 'concept' 'concern' 'datum' 'discipline' 'element' 'form' 'formal'\n",
            " 'fundamental' 'group' 'history' 'include' 'interact' 'kingdom'\n",
            " 'knowledge' 'make' 'matter' 'molecule' 'natural' 'practice' 'problem'\n",
            " 'process' 'program' 'reaction' 'science' 'scientific' 'structure'\n",
            " 'substance' 'theory' 'use' 'zoology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "สร้าง object for LDA model using gensim library"
      ],
      "metadata": {
        "id": "7OlmBpeHIcZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([ (feature_names[i] + \" (\" + str(topic[i].round(2)) + \")\")\n",
        "          for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
        "        for doc_index in top_doc_indices:\n",
        "            print(str(doc_index) + \". \" + documents[doc_index])\n"
      ],
      "metadata": {
        "id": "qeT2PR8mSrGN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set topic modeling algorithm arguments\n",
        "\n",
        "no_topics = 6 #@param {type:\"integer\"}\n",
        "\n",
        "# Run LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=no_topics,random_state=0).fit(tf)\n",
        "lda_W = lda_model.transform(tf)\n",
        "lda_H = lda_model.components_\n",
        "\n",
        "print(\"LDA Topics\")\n",
        "display_topics(lda_H, lda_W, tf_feature_names, data['Content'], no_top_words=3, no_top_documents=no_topics)\n",
        "# Visualization can be displayed in the notebook\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "pyLDAvis_data = pyLDAvis.lda_model.prepare(lda_model, tf, tf_vectorizer)\n",
        "\n",
        "# Visualization can be displayed in the notebook\n",
        "pyLDAvis.display(pyLDAvis_data)\n"
      ],
      "metadata": {
        "id": "RQKCcZhQC5OT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}